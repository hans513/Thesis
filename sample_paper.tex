\documentclass[twoside]{article}
\usepackage{aistats2014}
\usepackage{braket}
\usepackage[]{algorithm2e}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage[Symbol]{upgreek}
\usepackage{mathtools}

% If your paper is accepted, change the options for the package
% aistats2014 as follows:
%
%\usepackage[accepted]{aistats2014}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Instructions for paper submissions to AISTATS 2014}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
  The Abstract paragraph.
\end{abstract}

\section{INTRODUCTION}
% Hartree-Fock intro
In the fields of computational chemistry, Hartree-Fock is a method used to approximate the electron distribution of a molecule and the energy of the system. It is the cornerstone of an important problem in Quantum Chemistry: the energy minimization problem in which we want to discover the equilibrium geometry of a molecular system. 

%quantum chemistry studies the ground state, excited states and transition states of individual atoms and molecules, that occur during chemical reactions. 
%By calculating the energy of the molecular system with different geometry, we can discover the equilibrium geometry which is the one with minimum energy

Hartree-Fock is an iterative process which can be excessively time consuming or even infeasible if we want to get an accurate description of the electron distribution for large molecules. Speeding up the Hartree-Fock would save computational cost thus enable us to deal with more complicate molecules, which can in turn be utilized to study biochemical reaction with very large systems. 


[=== No Embedded parameter now!!! ===] \\

Tanha et al \cite{Matteus}, proposed a method for finding the similarities in molecules, in which parameters are embedded into Hartree-Fock model. The model is partitioned into few parts, including  kinetic energy, electron-nuclear interaction and electron-electron interaction, where each of these components have different parameters to make the whole process converge faster and output the correct density matrix.

With these embedded parameters,  we have the extra flexibility to change the behaviour of the Hartree-Fock method. This extra flexibility allows us to let each of the Hartree-Fock iteration make more progress. In particular, we would like to make single iteration of Hartree-Fock with embedded parameters achieve as much progress as two or even more iterations of the original Hartree-Fock.

[+++ No Embedded parameter now!!! +++ ] \\

% How to do that? (Imitation learning)
This idea can be realized through imitation learning, also called Learning from Demonstration, a paradigm designated to learn expert demonstrations so as to become capable of performing the same tasks. 
We can generate an illustrative sequence from the original Hartree-Fock as expert demonstrations. By imitation learning technique, we can make 
Hartree-Fock with embedded parameters learns from the demonstrations,  which finally enables it to gain more progress in one single iteration.


% Problem encountered   %%%%%
However, naive imitation learning may yield poor performance in theory and practice since the learner's prediction and action affect future input observations and states during the execution of the learned policy. It obviously violates the common i.i.d. assumption made in most statistical learning approaches. \cite{Ross}.


% The recipe (DAgger)
Fortunately, Ross et al,\cite{DAgger} proposed the dataset aggregation algorithm (DAgger), which learns a stationary deterministic policy guaranteed to perform well under its induced distribution of states, is a remedy to the poor performance in imitation learning. DAgger also was proven to have stable performance and fast learning rate. \cite{DAggerCompare}

In this paper,  we introduce the imitation learning technique for speeding up the Hartree-Fock process. 
Through dataset aggregation algorithm (DAgger) \cite{DAgger}, we can train a sequence of Hartree-Fock with embedded parameters that can perform well under the induced distribution of states and also yield more progress in each iteration. 

%introducing “parameter" into HF  turn it to Machine learning problem.

\section{BACKGROUND}

\subsection{Hartree-Fock}

% Goeff's comment
% We are not just care about computing the whole energy
% We also care about the sub-region energy and electron density directly
% The goal is to calculate the properties of the molecule such as the energy and density

In the area of theoretical chemistry, the main objective is to find a method that can calculate the electronic structure of any given molecular system. Efficiently computing the energy of a molecule as a function of its structure, E(R), is one of the most fundamental challenges in computational chemistry. Algorithms with sufficient accuracy are now available. In all these cases, Hartree-Fock (HF) algorithm is the starting point.



[== Talk about wave function before Schrödinger== ]
% Schrödinger equation describe how to get wave function
% Density matrix is a class of mixture of wave function 

% What is wave function, what is density matrix, 
% Why does Hartree-Fock work on density matrices instead of wave function
% It's hard to describe multiple particles by wave function
%  Particles are interchangeable so we can summrize it as density matrices
% Hartree-Fock works by repreatly updating the density matrices
The Schrödinger equation is the foundation equation of quantum mechanics, in which the solution can give us a complete description of a particle or a molecular systems. The equation gives the wave functions of a system which describes the quantum state of a system of one or more particles, and contains all the information about the system considered in isolation. A time invariant steady state form of the Schrödinger equation  is shown below
\[
				\hat{H}\Psi = E\Psi
\]
The $\hat{H}$ is the Hamiltonian operator which operates on the wave-function, $\Psi$, providing the observed energy, E. The Hamiltonian operator is given by the sum of two operators: $\hat{H} = \hat{T} + \hat{V}$,  in which $\hat{T}$ is the kinetic energy operator and $\hat{V}$ is the potential energy operator.


[Since we want to introduced wave function and density matrix before Schrödinger\\
=[====this paragraph should move up ====] \\
Hartree-Fock (HF) gives us an approximation for a solution to the Schrödinger equation. In HF the wave-function is described with a density matrix, which is a probability matrix of the mixed states within the system. The wave-function contains more information regarding the particle or system. However, it is difficult to construct a wave function for a many body system but approximating it a density matrix will allow us to calculate the approximated Schrödinger equation or Hartree-Fock. \\  

The general equation to formulate a density matrix is given by
\[
\rho = \sum\hat{p_i}\braket{\psi_i}\braket{\psi_i}
\]
This is an integral to find the probability of the different states, in which $\hat{p}$ is the density operator. Another approximation of Hartree-Fock is the usage of Linear Combination of Atomic Orbitals (LCAO) in which a molecular orbital is described by a sum of atomic orbitals. $\psi(r) = \sum{c_1\chi_i(r)}$


% Geoff's comment
% Gaussian is not an approximation of basis set
% we choose each basis element to be a linear combination of  Gaussian function (because of efficiency)
To computationally describe the atomic orbitals, basis functions are used and collectively they are called a basis set. Basis functions are best described using Slater functions/exponent. But because of the computationally difficulty of calculating the integral, a basis set of linear combinations of Gaussian functions is more computationally efficient. The more Gaussian functions per basis function the more accurate result we can attain. The larger the molecular system, the more accurate model is desired. Both will lead to the system growing exponentially large. The integrals of the these basis functions are viewed as matrices when solving the HF algorithm.

A modification of the Hamiltonian is approximated to the Fock operator which has the following form:
\[
\hat{F}(\rho) = \hat{h_1} + \hat{G}(\rho)
\]
where $\hat{h_1}$ is the one electron Hamiltonian containing the kinetic energy operator and the electron-nuclear interaction operator. $\hat{G}$ is the approximation of the two electron interaction which accounts for electron correlation. There are more develop methods based on HF that use a different models for the $\hat{G}$ operator to approximate more accurate electron correlation. The nuclear-nuclear interaction is approximated to constant so not accounted for in the Fock operator when solving the the Hartree-Fock algorithm. \\ 
The Fock matrix represented in the Roothaan equation is used to iteratively get the final density matrix.
\[
FC = SC\epsilon
\]

%Geoff's comment
%What is the ovelap matrix??

% We might not need to talk about how we get fock operator other than that is an function of the density matrix (Put it into Appendix) [too much details and distract]

% Use First person viewpont
% second person is distracting
where S is the overlap matrix and $\epsilon$ is a diagonal matrix of the orbital energies. This is similar to an eigenvalue equation with the exception of the existence of the S matrix. By preforming a transformation of the basis to get to an orthogonal basis will make S vanish leading to eigenvalue equation problem. However, since F depends on its own solution, this equation has to be done iteratively. The heart of the algorithm is solving for the eigenvalues in the Fock determinant as shown below.\\
\begin{eqnarray}
0 =
\left|
\begin{array}{ccc}
\textbf{F}-\epsilon &\textbf{F}&\textbf{F}\\
\textbf{F}&\ \textbf{F}-\epsilon &\textbf{F}\\
\textbf{F}&\textbf{F}&\textbf{F}-\epsilon \\
\end{array}
\right|
\end{eqnarray}

% Make claer what  the input and oput are
% single electron density matrix is the input and output 

The core equation in the Hartree-Fock algorithm is as following: \\
\begin{algorithm}[htb]
 \KwData{ }
 \KwResult{Converged density matrix }
	Set up the basis set\\
	Get integrals for Hamiltonian (H1,H2)\\
	Pick a guess density $\rho = \rho_0$ \\
	Set $\delta$ to be termination criteria \\
 \While{ $E_{i-1}(\rho)-E_i(\rho) > \delta$ }{
	Calculate the Fock operator F($\rho$) = $h_1 + G(\rho)$\\
	Diagonalize the Fock matrix to get $\epsilon$\\
	Get new density from the coefficients C $\rightarrow$ $\rho_i$ \\
	Set the new density $\rho = \rho_i$\\
 }
 \caption{Hartree-Fock algorithm}
\end{algorithm}
 

% Some intuition about these matrices
% Run Hartree to get the eigen states of h2. 
% show the first few eigen states (Schrödinger) of the h2 molecules and describe how the density matrices correspond to that plot (picture) to make things more concrete 



In other words, we need to have the Fock-orbitals to get the Fock operator. To solve this equation, we start with an arbitrary guess density and solve for the Fock operator. We will then get the new density which we again will use it to solve the new Fock operator again. We will keep doing this process until meet the termination criteria. In large molecular system, the process is tedious and each iteration may take hours or even days of computation thus become infeasible.


\subsection{Speeding up Hartree-Fock as an imitation learning problem}


Accelerating Hartree-Fock convergence has previously been explored in the matter of creating a distribution of the result from previous iterations to predict the next\cite{Pulay1980}. 

.....The drawback of the research......

In this paper,
we want to explore a different method where to find parameters which are embedded in the Hamiltonian that can speed this iterative algorithm. This could perhaps also lead to finding a good initial guess density, which could speed this method significantly. 


%We are exploring a different method where to find parameters which are embedded in the Hamiltonian that can speed this iterative algorithm. This could perhaps also lead to finding a good initial guess density, which could speed this method significantly. 


% The difficulty of (speeding up the whole process)parallelling the whole procedure, therefore, we'd like to speed up single iteration 
%% In general, we can think of two approaches to speed up a process. The first approach would be paralleled the whole procedure.  The whole process is hard to paralleled, since the input of each iteration depend on the output of previous iteration.


% Description of the original iterative Hartree-Fock 
% This part (Should go earlier under the description of Hartree-Fock)

%How to map the Hartree-Fock to imitation learning problem.
As we mentioned before,  because of the Fock operator can only be calculated iteratively, Hartree-Fock is also an iterative process that  approaches the steady density matrix we want.

Given an initial guessed density matrix $d_0$ as an input for solving Fock operator, after first iteration we will get the a new output density matrix $d_1$. 
In the second iteration, the previous output density matrix will then become the input. Thus, using $d_1$ as the input, we can get the output density matrix $d_2$. 

We will keep doing this procedure until the difference between the input density matrix in the iteration $n$ $d_{n-1}$  and the output density matrix $d_n$ is less than a threshold, say $10^{-6}$. 

This repeating process is quite tedious and each iteration may take hours or even days. % Shorten each iteration?

If we can make each iteration more productive, we can achieve the same goal with less iterations. 

We can think the whole n iterations of the Hartree-Fock process as a sequence, beginning from initial density matrix $d_0$ through n-1 intermediate density matrices $d_1$,  $d_2$,  ...... $d_{n-1}$ and then finally get the steady converged output $d_{n}$. However, we indeed don't care about the intermediate density matrices. What the only thing we want to know is the final converged density matrix. If we can get the final converged density matrices with less iterations or even one iteration of computation that would greatly shorten the computation time and thus speed up the whole process. 

If we think of the sequence of density matrices as a film playing frame by frame. Then it is intuitive to think of a 2X speeding up version 
Under the same frame rate, the sequence of a  "2X fast forward" version would be dropping the odd numbered frames, thus the sequence will become $d_0$ to $d_2$, $d_2$ to $d_4$, ......, $d_{n-2}$ to $d_n$. In this ideal fast forward version, we will only need half iterations to achieve the same goal, thus cutting the computation cost into half.


This sequence can serve as an expert demonstration, and the extra flexibility offered by the embedded parameter can let us reform the Hartree-Fock process. By imitation learning,
we would like to train a Hartree-Fock process with embedded parameter that  mimics the "Fast forward" version of the original Hartree-Fock. 
We can always try to make the output density matrix match the ideal output in our expert demonstration via the extra flexibility offered by the embedded parameters .


As a result, it allows more progress in one iteration, thus converged with less iterations, say half or even less iterations.  

%The present work therefore intends to treat the Hartree-Fock process as an expert demonstration, and we apply DAgger algorithm to imitate the process for learning a fast-forward version of Hartree-Fock.


%if you want to use more Gaussian functions to get a more accurate description of the orbitals. 

%Hartree-Fock is a method used to get the density matrix for a given molecule. The density matrix describes the molecule and the probability distribution of the location of the electrons.

 %Hartree-Fock is an approximation for solving the Schrödinger equation where it assumes that the wave function can be approximated by a single Slater determinant. The electrons or orbitals are described with a combination of Gaussian functions.

%Hartree-Fock
%density matrix
%Gaussian  , wave function, basis
%Slater determinant


\subsection{DAgger algorithm}
% sovle the naive imitation learning problem
DAgger is an algorithm that learns expert demonstration iteratively. In each iteration, the model is trained under the states that was induced by both the expert and the previous learned model. It keeps aggregating a set of inputs that the model itself is likely to encounter based on previous training iterations.  

By doing so, it is possible to offset the error made by previous learned model and thus learn a better model. This is a remedy to the problem in naive imitation learning that the result may become unpredictable and poor because of the policy is trained under a different state distribution
%!!!!!!!!!!!!!!!!!!!!!!
that the model is less likely to encounter. 


Thus the error may keep growing iteration by iteration.

%Rough Algorithm Description
The simplest DAgger algorithm may proceed as follows.
%%% FROM Original paper
%%% TODO 
$\Pi$ is the class of policies the learner is considering.
In the first iteration, it uses the expert’s policy $\pi^*$ to gather
a dataset of trajectories D and train a policy $\hat{\pi_2}$ that best mimics the expert on those trajectories. 

Then in iteration $i$, we can sample the state induced by $\hat{\pi_i}$ itself and refer to expert's action on these states, forming the dataset $D_i$, which is in turn added to dataset D. We then train the next policy $\hat{\pi_{i+1}}$ , the policy that best mimics the expert on the whole dataset D. The process is then repeated to further rectify the error produced by the policy learned in the previous iteration until we reach iteration N.

In other words, Dagger proceeds by aggregating a dataset
at each iteration under the current policy and trains the next policy under the all collected datasets. 


In general, there are two different strategies to learn the policy. Either we can try to learn a single policy that imitate the expert's demonstration under all circumstances, or we train multiple policies that approximate expert's demonstration in which each policy best mimic expert's demonstration under different scenario.
It's obvious that using multiple policies gives us more flexibility, thus in the following section, we would like to introduce how we train a sequence of Hartree-Fock with embedded parameters, in which each set of parameter can best speed up a correspondent iteration.



%However, Like most of the iterative algorithm, the result may oscillate dramatically in the very first few iterations then getting stable and finally become converged after a bunch of iterations. 



%For example, during training one
%can use a “mixture oracle” that at times takes an action given by the previous learned policy [11].
%Alternatively, at each iteration one can learn a policy from trajectories generated by all previous policies [3].


\begin{algorithm}[htb]
 \KwData{ }
 \KwResult{Best $\hat{\pi}_i$ on validation }
 $\pi^*$  is the expert’s policy \\
 Initialized $D \leftarrow \emptyset$ \\
 Initialized $\hat{\pi}_1$ to any policy in $\Pi$ \\
 \For{i=1 to N }{
	Let $\pi_i$ = $\beta_{i}\pi^* + (1-\beta)\hat{\pi_i}$ \\
	Sample T-step trajectories using $\pi_i$ \\
	Get dataset $D_i$ = \{(s, $\pi^*$(s))\} of visited states by $\pi_i$ and actions given by expert. \\
	Aggregate datasets: $D \leftarrow D \cup D_i$ \\
	Train classifier $\hat{\pi}_{i+1}$ on $D$\\
 }
 \caption{DAgger algorithms}
\end{algorithm}

% Connecting the symbol in algorithm to our scenario

%Dagger is a powerful algorithm that can train a model which performs well in its the induced state.

\section{LEARNING EMBEDDED PARAMETERS}

Our goal is to have the Hartree-Fock with embedded parameters achieve the same goal as original Hartree-Fock with less iterations, say, half or even less.  


% The original Hartree-Fock is actually a Hartree-Fock with all-zero embedded paramter

% In our scenario the 2X fast forward version of Hartree-Fock is \pi^*
% D is the training dataset that compose of the input density matrices and correspondent ideal output density matrices
% 

With the fast forwarded trajectories to serve as expert's demonstration, in each single iteration $i$ in Hartree-Fock, we would like to apply DAgger to learn a Hatree-Fock with embedded parameter $\uppi_i$ that best speeds up the i-th iteration.   

In our case, $\Pi$ is the class of all the Hartree-Fock with embedded parameters. $\pi^*$ is the expert's demonstration, given by the original Hartree-Fock. We denote $\hat{\pi}_{(i,j)}$  the policy trained in the Hartree-Fock iteration $i$ and DAgger iteration $j-1$. 


% Geoff: introduce validation until latter
When applying DAgger to Hartree-Fock iteration $i$,  we initialize $\pi_{(i,1)}$ equal to $\pi^*$. We'll then generate a sequence of policies $\hat{\pi}_{(i,2)} , \hat{\pi}_{(i,3)}, \ldots, \hat{\pi}_{(i,i+1)}$. The policy $\uppi_i$  is the last policy in this sequence.

%The policy $\uppi_i$  is the best policy on validation among $\hat{\pi}_{(i,*)}$. 


% Why train \pi_{i,j} need \pi{i-1, j-1}
 After training we'll have a sequence of policies $\uppi_1, \uppi_2, \ldots, \uppi_n$ in which the output of previous policy will become the input of next policy.

The idea of DAgger is to train the policy under the induced state of the previous policy. Therefore, when training policy $\pi_{(i,j)}$ in Hartree-Fock iteration $i$, the previous policy is $\hat{\pi}_{(i-1,j-1)}$ instead of  $\hat{\pi}_{(i,j-1)}$.

We can visualize the whole process as a two-dimension table. 
In this table, each row represents the proceeding of Hartree-Fock and each column represents the aggregating learning process of DAgger.
 
Assume the density matrices generated in the original Hartree-Fock process in sequence are  $d_0 \rightarrow  d_1 \rightarrow  d_2  \ldots  $ and finally converge at $d_{2n}$. The expert's demonstration will then be $d_0 \rightarrow d_2 \rightarrow  d_4 \rightarrow  \ldots \rightarrow  d_{2n}$

In the first iteration of Hartree-Fock, we only has one DAgger iteration, since there is no previous Hartree-Fock iteration. 

Our only objective here is to train a set of parameter that best matches density matrix $d_0 \rightarrow d_2$. 

%TODO: Error of density matrix
% Geoff: define d2' in terms of pi hat(1,2)
Say, after training, we got the policy $\hat{\pi}_{(1,2)}$: $d_0 \xrightarrow{\hat{\pi}_{(1,2)}} d_{2}'$ 
%which in turn can map density matrix 

Then for the iterations afterwards, we have two sources of the training examples.  One from the expert's demonstration (All the objective listed in the first row of the DAgger iteration: iter 1), and the other  is the state which is induced by the previous learned policy (The new objective which is generated after the first DAgger iteration). Thus we can introduce DAgger to learn a best Hartree-Fock with embedded parameters for each iteration afterwards.

In the second iteration of Hatree-Fock, we'll have two DAgger iterations. In the first DAgger iteration, the only training objective is from expert's demonstration, and we want to train a set of parameter that best matches density matrix $d_2 \rightarrow d_4$.

However, in the second DAgger iteration, we'll sample the trajectories using $\hat{\pi}_{(1,2)}$, thus getting the dataset $D_i$ = $d_2' \rightarrow d_4$. Then the dataset is added into D.
%We then aggregate dataset that is the state induced by iteration 1,
Next step is to train a policy that best fit the aggregated dataset D. Thus,  not only do we want to fit the expert's demonstration $d_2 \rightarrow d_4$, but also want to fit the output of the previous iteration $d_{2}' \rightarrow d_4$. 
%The result of this policy then is the best policy that mimic the expert's demonstration in Hartree-Fock iterations two.

This is the key to compensating the error made by the previous iteration. It will keep aggregating the output of the previous iterations.

%In the Hartree-Fock iteration $i$,  we'll apply DAgger algorithm to train a policy that best fit that iteration. Thus in Dagger iteration j, $j<i$, we'll first sample the trajectories using $\hat{\pi}_{i-1,j}$. That is the result after training in Hartree-Fock iteration $i-1$, Dagger iteration $j-1$. 


% Goeff: need a new section





%Then $\hat{\pi}_{(i,i+1)}$ is the last policy we can get in the Hartree-Fock iteration $i$.



we'll  sample the trajectories using $\hat{\pi}_{(i-1,j-1)}$. We then add the sample into the aggregated dataset D.
We want to train a policy that best fits all the density matrices in D to the ideal training objective.

As a result, we'll fit the expert's demonstration $d_{2(i-1)}$ to $d_{2i}$, and also  all the possible output of the previous iteration $d_{2(i-1)}'$,  $d_{2(i-1)}''$, $\ldots$,  $d_{2(i-1)}^{[n-1]}$  to the ideal training objective $d_{2i}$.

After training, for each iteration, we'll get a best set of parameter $\uppi$ that not only mimics the expert's demonstration in that iteration but also offsets the error made by the previous iteration.

% 
Put these $n$ Hartree-Fock with embedded parameters $\uppi_1, \uppi_2, \ldots, \uppi_n$ in sequence, we'll have a new process that simulate the original Hatree-Fock but may get converged density matrix with only half of iterations. In this new process, the output of previous policy would become the input of next policy just like a pipeline. 

%Feed the initial guessed density matrix $d_0$ into $\uppi_1$, we'll get the output density matrix $d_2'$. This will then be feed into $\uppi_2$.


\begin{center} 
\begin{table*}[t]
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
& \multicolumn{7}{l|}{Hartree-Fock iterations (with step size 2)}                                                                                                                    \\ \hline
\multirow{10}{*}{\begin{tabular}[c]{@{}l@{}}DAgger\\ iterations\end{tabular}} 
&                &                & iter 1         & iter 2          & iter 3         & \ldots         & iter n         \\ \cline{2-8} 
& \multirow{2}{*}{iter 1} 
& Objective & $d_0 \rightarrow d_2$ & $d_2 \rightarrow d_4$ & $d_4 \rightarrow d_6$ &  \ldots & $d_{2(n-1)} \rightarrow d_{2n}$ \\ \cline{3-8} 
&                 & Result after training & $d_0 \rightarrow d_2'$ & $d_2 \rightarrow d_4'$   & $d_4 \rightarrow d_6'$    &  \ldots & $d_{2(n-1)} \rightarrow d_{2n}'$          \\ \cline{2-8} 
& \multirow{2}{*}{iter 2} & New objective         &                         & $d_2' \rightarrow d_4$   & $d_4' \rightarrow d_6$     &  \ldots & $d_{2(n-1)}' \rightarrow d_{2n}$          \\ \cline{3-8} 
&                 & Result after training &                 & $d_2' \rightarrow d_4''$ & $d_4' \rightarrow d_6''$   & \ldots & $d_{2(n-1)}' \rightarrow d_{2n}''$        \\ \cline{2-8} 
& \multirow{2}{*}{iter 3} & New objective         &                         &                          & $d_4'' \rightarrow d_6$    &  \ldots & $d_{2(n-1)}'' \rightarrow d_{2n}$         \\ \cline{3-8} 
&                 & Result after training &                 &                 & $d_4'' \rightarrow d_6'''$ &  \ldots & $d_{2(n-1)}'' \rightarrow d_{2n}'''$      \\ \cline{2-8} 
& \vdots      & \vdots      &                &                &                & $\ddots$ &   \vdots \\ \cline{2-8} 
& \multirow{2}{*}{iter n} & New objective         &                         &                          &                            &  & $d_{2(n-1)}^{[n-1]} \rightarrow d_{2n}$     \\ \cline{3-8} 
&                & Result after training &                &                &                &  & $d_{2(n-1)}^{[n-1]} \rightarrow d_{2n}^{n}$ \\ \hline
\end{tabular}
\end{table*}
\end{center} 
\section{EXPERIMENTS}

In energy minimization, we want to discover a geometry of a particular arrangement of the atoms that corresponds to global energy minimum. This geometry is called the equilibrium structure of a molecule.
Hartree-Fock plays a crucial role in calculating the energy of the system under different geometries. 

In the experiment, we have the dataset which is 
 a set of molecules with different geometries in a range of environments.
 We already have the energy of each configuration in the dataset, we want to evaluate the computation time of the 
 
 
\subsection{Experiment Dataset}

The data consists of the electronic structure of a diatomic molecules/ hydrocarbon with varying geometries in a range of environments. 

% 10 different geometry
To generate different geometries,
a random number is added to each value in the bond length, bond angle and dihedral angle, using a uniform random distribution with a width of ±0.2 Å for bond lengths, ±10o for bond angles, and free rotation for dihedral angles. 

The random variable is chosen to span the full range for the internal rotation angle. The environment perturbs the electronic structure of the fragment in a manner that explores the types of perturbations that will be present in large molecules. 

%This includes perturbations from external electrostatic potentials, due to other portions of the molecule or from solvent, and inductive effects from acceptors and donors. 

%Environment
The environments consist of a cube, each corner of which holds a point charge. The length of each side of the cube is 12Å for methane and ethane and 14Å for propane and butane, with the molecule placed at its center. 

[The relation between the geometry and environment]

% Point charges
%The magnitudes of the point charges are randomly generated using a uniform distribution between -25 and 25 amu, chosen to induce variations in the Mulliken charges on the C and H in methane and ethane that are similar to the charges induced on the methyl group in CF3CH3 (~0.2 amu).  


For each pairing of a molecular configuration with an environment, we generate expectation values of each operator that appears in the Hamiltonian (total kinetic energy, interaction of the electron density with each nucleus, and total two-electron repulsion energy).


% DATA SET
Each data set consists of 10 configurations in 10 environments, corresponding to 100 calculations. Ethane has eight nuclei which, along with kinetic energy and two-electron energy, leads to 1000 data points. 

[]



% 30 iterations

\subsection{Experiment settings}


%We generate a few hundred molecules dataset and divide these between a training and a test data set.
%1 etot 
%2 convergence rate (L1 norm with previous iteration)

We split the whole dataset into two part evenly. 
[What is the training dataset]
[What is the testing dataset]
%The odd numbered instances serve as training data, on the other hand the even numbered instances serve as testing data. \\

% Goal
[GOAL]
The goal is to train 15 Hartree-Fock with embedded parameters that best approximate 30 iterations of original Hartree-Fock.

After training on training dataset, we'll apply these 15 Hartree-Fock with embedded parameters on testing dataset to evaluate the performance.

How  it converges
The lower the energy the better?
%What is convergence rate \\
We can also plot the difference between this iteration and previous iteration to observe the converge rate.

What is "etot"  and Energy\\

\subsection{Experiment result}
[CLAIM]


\section{GENERAL FORMATTING INSTRUCTIONS}



Both submitted and camera-ready versions of the paper are 8 pages,
plus any additional pages needed for references.

Papers are in 2 columns with the overall line width of 6.75~inches (41~picas). Each column is 3.25~inches wide (19.5~picas).  The space
between the columns is .25~inches wide (1.5~picas).  The left margin is 1~inch (6~picas).  Use 10~point type with a vertical spacing of
11~points. Times Roman is the preferred typeface throughout.

Paper title is 16~point, caps/lc, bold, centered between 2~horizontal rules.  Top rule is 4~points thick and bottom rule is 1~point thick.
Allow 1/4~inch space above and below title to rules.

Author descriptions are center-justified, initial caps.  The lead
author is to be listed first (left-most), and the Co-authors are set
to follow.  If up to three authors, use a single row of author
descriptions, each one center-justified, and all set side by side;
with more authors or unusually long names or institutions, use more
rows.  (But, do not include author names in the initial double-blind
submission!  Instead leave a row of ``Anonymous Author'' descriptions
as above.)


One-half line space between paragraphs, with no indent.

\section{FIRST LEVEL HEADINGS}

First level headings are all caps, flush left, bold, and in point size
12. One line space before the first level heading and 1/2~line space
after the first level heading.

\subsection{Second Level Heading}

Second level headings are initial caps, flush left, bold, and in point
size 10. One line space before the second level heading and 1/2~line
space after the second level heading.

\subsubsection{Third Level Heading}

Third level headings are flush left, initial caps, bold, and in point
size 10. One line space before the third level heading and 1/2~line
space after the third level heading.

\paragraph{Fourth Level Heading}

Fourth level headings must be flush left, initial caps, bold, and
Roman type.  One line space before the fourth level heading, and
place the section text immediately after the heading with, no line
break, but an 11 point horizontal space.

\subsection{CITATIONS, FIGURES, REFERENCES}


\subsubsection{Citations in Text}

Citations within the text should include the author's last name and
year, e.g., (Cheesman, 1985). References should follow any style that
you are used to using, as long as their style is consistent throughout
the paper.  Be sure that the sentence reads correctly if the citation
is deleted: e.g., instead of ``As described by (Cheesman, 1985), we
first frobulate the widgets,'' write ``As described by Cheesman
(1985), we first frobulate the widgets.''  Be sure to avoid
accidentally disclosing author identities through citations.

\subsubsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first
  footnote.} in the text. Use 8 point type for footnotes. Place the
footnotes at the bottom of the column in which their markers appear,
continuing to the next column if required. Precede the footnote
section of a column with a 0.5 point horizontal rule 1~inch (6~picas)
long.\footnote{Sample of the second footnote.}

\subsubsection{Figures}

All artwork must be centered, neat, clean, and legible.  All lines
should be very dark for purposes of reproduction, and art work should
not be hand-drawn.  Figures may appear at the top of a column, at the
top of a page spanning multiple columns, inline within a column, or
with text wrapped around them, but the figure number and caption
always appear immediately below the figure.  Leave 2 line spaces
between the figure and the caption. The figure caption is initial caps
and each figure should be numbered consecutively.

Make sure that the figure caption does not get separated from the
figure. Leave extra white space at the bottom of the page rather than
splitting the figure and figure caption.
\begin{figure}[h]
\vspace{.3in}
\centerline{\fbox{This figure intentionally left non-blank}}
\vspace{.3in}
\caption{Sample Figure Caption}
\end{figure}

\subsubsection{Tables}

All tables must be centered, neat, clean, and legible. Do not use hand-drawn tables. Table number and title always appear above the table.
See Table~\ref{sample-table}.

One line space before the table title, one line space after the table title, and one line space after the table. The table title must be
initial caps and each table numbered consecutively.

\begin{table}[h]
\caption{Sample Table Title} \label{sample-table}
\begin{center}
\begin{tabular}{ll}
{\bf PART}  &{\bf DESCRIPTION} \\
\hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{SUPPLEMENTARY MATERIAL}

If you need to include additional appendices during submission, you
can include them in the supplementary material file.


\newpage

\section{INSTRUCTIONS FOR CAMERA-READY PAPERS}

For the camera-ready paper, if you are using \LaTeX, please make sure
that you follow these instructions.  (If you are not using \LaTeX,
please make sure to achieve the same effect using your chosen
typesetting package.)

\begin{enumerate}
    \item Install the package \texttt{fancyhdr.sty}. The
    \texttt{aistats2014.sty} file will make use of it.
    \item Begin your document with
    \begin{flushleft}
    \texttt{\textbackslash documentclass[twoside]\{article\}}\\
    \texttt{\textbackslash usepackage[accepted]\{aistats2014\}}
    \end{flushleft}
    The \texttt{twoside} option for the class article allows the
    package \texttt{fancyhdr.sty} to include headings for even and odd
    numbered pages. The option \texttt{accepted} for the package
    \texttt{aistats2014.sty} will write a copyright notice at the end of
    the first column of the first page. This option will also print
    headings for the paper.  For the \emph{even} pages, the title of
    the paper will be used as heading and for \emph{odd} pages the
    author names will be used as heading.  If the title of the paper
    is too long or the number of authors is too large, the style will
    print a warning message as heading. If this happens additional
    commands can be used to place as headings shorter versions of the
    title and the author names. This is explained in the next point.
    \item  If you get warning messages as described above, then
    immediately after $\texttt{\textbackslash
    begin\{document\}}$, write
    \begin{flushleft}
    \texttt{\textbackslash runningtitle\{Provide here an alternative shorter version of the title of your
    paper\}}\\
    \texttt{\textbackslash runningauthor\{Provide here the surnames of the authors of your paper, all separated by
    commas\}}
    \end{flushleft}
    The text that appears as argument in \texttt{\textbackslash
      runningtitle} will be printed as a heading in the \emph{even}
    pages. The text that appears as argument in \texttt{\textbackslash
      runningauthor} will be printed as a heading in the \emph{odd}
    pages.  If even the author surnames do not fit, it is acceptable
    to give a subset of author names followed by ``et al.''

    \item Use the file sample\_paper.tex as an example.

    \item Both submitted and camera-ready versions of the paper are 8
      pages, plus any additional pages needed for references.

    \item If you need to include additional appendices,
      you can include them in the supplementary
      material file.

    \item Please, don't change the layout given by the above
      instructions and by the style file.

\end{enumerate}

\subsubsection*{Acknowledgements}

Use unnumbered third level headings for the acknowledgements.  All
acknowledgements go at the end of the paper.  Be sure to omit any
identifying information in the initial double-blind submission!


\subsubsection*{References}

References follow the acknowledgements.  Use an unnumbered third level
heading for the references section.  Any choice of citation style is
acceptable as long as you are consistent.  Please use the same font
size for references as for the body of the paper---remember that
references do not count against your page length total.

J.~Alspector, B.~Gupta, and R.~B.~Allen (1989). Performance of a
stochastic learning microchip.  In D. S. Touretzky (ed.), {\it
  Advances in Neural Information Processing Systems 1}, 748-760.  San
Mateo, Calif.: Morgan Kaufmann.

F.~Rosenblatt (1962). {\it Principles of Neurodynamics.} Washington,
D.C.: Spartan Books.

G.~Tesauro (1989). Neurogammon wins computer Olympiad.  {\it Neural
  Computation} {\bf 1}(3):321-323.

\begin{thebibliography}{9}
\bibitem{Matteus}
  Matteus Tanha, Shiva Kaul, Alex Cappiello, Geoffrey J. Gordon, David J. Yaron.
  \emph{Embedding parameters in ab initio theory to develop well-controlled approximations based on molecular similarity}.
  Technical report arXiv:1311.3440.
  
  \bibitem{DAgger}
  Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell.
  \emph{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}.
  Technical report arXiv:1011.0686, arXiv, 2011.
  
  \bibitem{DAggerCompare}
  Andreas Vlachos.
  \emph{An investigation of imitation learning algorithms for structured prediction}.
  
  \bibitem{Ross}
    Ross and J. A. Bagnell.
  \emph{Efficient reductions for imitation
learning.} Proceedings of the 13th International
Conference on Artificial Intelligence and Statistics (AISTATS),
2010.

  \bibitem{Pulay1980}
   Péter Pulay,
  \emph{Convergence acceleration of iterative sequences. The case of SCF iteration.} Chem. Phys. Lett. 73, 393 (1980).

\end{thebibliography}

\end{document}
