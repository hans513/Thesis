\documentclass[twoside]{article}
\usepackage{aistats2014}
\usepackage{braket}
\usepackage[]{algorithm2e}
% If your paper is accepted, change the options for the package
% aistats2014 as follows:
%
%\usepackage[accepted]{aistats2014}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Instructions for paper submissions to AISTATS 2014}

\aistatsauthor{ Anonymous Author 1 \And Anonymous Author 2 \And Anonymous Author 3 }

\aistatsaddress{ Unknown Institution 1 \And Unknown Institution 2 \And Unknown Institution 3 } ]

\begin{abstract}
  The Abstract paragraph.
\end{abstract}

\section{INTRODUCTION}
% Hatree-Fock intro
In the fields of computational chemistry and physics, Hartree-Fock is a method used to approximate the electronic distribution of a molecule and the energy of the system. Hartree-Fock is an iterative process which can be quite tedious and time-consuming for large molecules especially if we want to get an accurate description of the electron distribution. Speeding up the Hatree-Fock would thus enable us to deal with larger molecules, which can in turn be utilized to study biochemical reaction with very large systems. In essence, speeding up the Hatree-Fock algorithm will save computational cost to solve quantum chemical problems. 



% Parameters
Matteus et al, \cite{Matteus}, proposed a way to embed the parameters in the hamiltonian. This model is constructed to find the similarities in molecules. The hamiltonian is partitioned into kinetic energy, electron-nuclear interaction and electron-electron interaction, where each of these components have different parameters. The model also has different parameters for on atom and for bonds.


The parameters give us the flexibility to control the behaviour of the Hatree-Fock method. This flexibility allows us to let each of the Hatree-Fock iteration makes more progress. In particular, we would like to make single iteration of Hatree-Fock with parameters achieve as much progress as two or even more iterations of the original Hatree-Fock.


% How to do that? (Imitation learning)
This idea can be realized through imitation learning, also called Learning from Demonstration, a paradigm designated to learn expert demonstrations so as to become capable of performing particular tasks. By imitation learning, we can make Hatree-Fock with parameters learn from the original Hatree-Fock, which finally enables it to gain two iterations or more progress in one single iteration.


% Problem encountered   %%%%%
However, naive imitation learning may yield poor performance in theory and practice since the learner's prediction and action affect future input observations and states during the execution of the learned policy. It obviously violates the common i.i.d. assumption made in most statistical learning approaches. \cite{Ross}.


% The recipe (DAgger)
Fortunately, Ross et al,\cite{DAgger} proposed that the dataset aggregation algorithm (DAgger), which learns a stationary deterministic policy guaranteed to perform well under its induced distribution of states, is a remedy to the poor performance in imitation learning.


In this paper,  we introduce the imitation learning technique to speed up the Hartree-Fock process with dataset aggregation algorithm (DAgger) \cite{DAgger}, which was proven to have stable performance and fast learning rate. \cite{DAggerCompare}

%introducing “parameter" into HF  turn it to Machine learning problem.
%adjust the parameter that make it happend

\section{BACKGROUND}
\subsection{Hatree-Fock}

In the area of theoretical chemistry the main objective is to find a method to calculate the electronic structure of any given molecular system. Efficiently computing the energy of a molecule as a function of its structure, E(R), is one of the fundamental challenges of chemistry. Algorithms with sufficient accuracy are now available, where in all these cases Hartree-Fock algorithm is the initial starting point. Hartree-Fock is an approximation for solving the Schrödinger equation where it assumes that the wave function can be approximated by a single Slater determinant[ref]. The general notation for getting the energy in quantum chemistry is as shown below:

%$H\Psi = E\Psi$

E = $\Bra{\Psi}H\Ket{\Psi}$ ,

where H is the hamiltonian operator, $\Psi$ is a wave function for any given molecule and E is the energy of the molecule. The general form of the Hamiltonian consists of kinetic energy operator, electron-nuclear interaction operator, electron-electron interaction operator and nuclear-nuclear interaction operator. The Hamiltonian is approximated to the Fock operator which is shown below:

$\hat{F}$ = $\hat{h_1} + \sum\limits_{i}(\hat{J_i} - \hat{K_i})$,

where $\hat{h_1}$ is the one electron hamiltoninan containing the kinetic energy operator and the electron-nuclear interaction operator. $\hat{J}$ and $\hat{K}$ are components of the electron-electron interaction operator given in the same dimension as the one electron operator. The nuclear-nuclear interaction is approximated to constant so not accounted for in the Fock operator when solving the the Hartree-Fock algorithm. The wave function is approximated to a density matrix with a set of gaussian functions for each atom in the molecular system, which is denoted as C. The core equation in the Hartree-Fock algorithm is as following:

FC = SC$\epsilon$,

where S is the overlap matrix and $\epsilon$ is a diagonal matrix of the orbital energies. This is similar to an eigenvalue equation with the exception of the existence of the S matrix. By preforming a transformation of the basis to get to an orthogonal basis will make S vanish leading to eigenvalue equation problem. However, since F depends on its own solution this equation has to be done iteratively. In other words you need to have the Fock-orbitals to get the Fock operator. 
To solve this equation you start with an arbitrary guess density and then solve for the fock operator and then get the new density which you again will use to solve to get the new Fock operator and so on. 

The cost of the methods based on Hartree-Fock algorithm scales as N\textsuperscript{p} where: 

N = Number of basis functions (features) 

= n Natoms  (n = basis functions/atom)  

p  = depending on the accuracy model between 3.5 and 8\cite{Fisch1996}

Accelerating Hartree-Fock convergence has previously been explored in the matter of creating a distribution of the result from previous iterations to predict the next\cite{Pulay}. We are exploring a different method where to find parameters which are embedded in the hamiltonian that can speed this iterative algorithm. This could perhaps also lead to finding a good initial guess density, which could speed this method significantly. 

%Hatree-Fock
%density matrix
%Gaussian  , wave function, basis
%Slater determinant


\subsection{Speeding up as imitation learning}
% Description of the original iterative Hatree-Fock
Description of the original iterative Hatree-Fock \\


The whole Hatree-Fock process is pretty time consuming especially for large molecular system, each iteration may cost hours of computation. 

% The difficulty of (speeding up the whole process)parallelling the whole procedure, therefore, we'd like to speed up single iteration 
In general, we can think of two approaches to speed up a process. The first approach would be paralleled the whole procedure.
The whole process is hard to paralleled, since the input of each iteration depend on the output of previous iteration.


The second approach would be improving the procedure, make each iteration more productive so that we can achieve the same goal with less iterations. 

%How to map the Hatree-Fock to imitation learning problem.
% The idea is simple
Hatree-Fock is an iterative process. Given an initial density matrix $d_0$ as an input to the Hatree-Fock process, after first iteration we'll get the output density matrix $d_1$. In the second iteration, the previous output density matrix will then become the input, thus feeding in $d_1$ we can get the output density matrix $d_2$. We'll keep doing this procedure until the difference between the input density matrix in the iteration $n$ $d_{n-1}$  and the output density matrix $d_n$ is less than a threshold, say $10^{-6}$. 

As a result, we can think whole n iterations of original Hatree-Fock process as a step by step flow, from initial density matrix $d_0$ through n-1 intermediate density matrices $d_1 d_2 ...... d_{n-1}$ , then finally get the converged output $d_{n}$ just like a film playing frame by frame. 

With imitation learning,
we can treat the original Hatree-Fock iteration as the expert demonstration. However, instead of learning the original Hatree-Fock process, we would like to learn a "Fast forward" version of Hatree-Fock. We can always try to make the output density matrix match the ideal output in our training data.

It's intuitive to think of a "2X fast forward" version of Hatree-Fock which would be dropping the odd numbered frame, thus the procedure will become mapping $d_0$ to $d_2$, $d_2$ to $d_4$, ......, $d_{n-2}$ to $d_n$. In this fast forward version,
we'll only need $frac{n}{2}$ iterations, thus the computation is roughly cut into half.

With embedded parameters and the extra flexibility,
our goal here is to learn a Hatree-Fock with parameter which can make more progress in one iteration, thus converge with less iterations, say half or even less iterations.  

%The present work therefore intends to treat the Hartree-Fock process as an expert demonstration, and we apply DAgger algorithm to imitate the process for learning a fast-forward version of Hartree-Fock.


%if you want to use more Gaussian functions to get a more accurate description of the orbitals. 

%Hartree-Fock is a method used to get the density matrix for a given molecule. The density matrix describes the molecule and the probability distribution of the location of the electrons.

 %Hartree-Fock is an approximation for solving the Schrödinger equation where it assumes that the wave function can be approximated by a single Slater determinant. The electrons or orbitals are described with a combination of Gaussian functions.

%Hatree-Fock
%density matrix
%Gaussian  , wave function, basis
%Slater determinant


\subsection{DAgger algorithm}
% sovle the naive imitation learning problem
DAgger is an iterative algorithm that learns expert demonstration iteratively. In each iteration, it was trained under the states that may be induced by both the expert and the learner itself. It builds up the set of inputs that the learned policy is likely to encounter based on previous training iterations, thus it's possible to correct the error made by previous iteration. This is a remedy to the problem in naive imitation learning that the result may become unpredictable and poor because of the policy is trained under a different state distribution. Thus the error may keep growing iteration by iteration.

The simplest DAgger algorithm may proceed as follows.


... Rough Algorithm Description goes here.....

%Rough Algorithm Description
%by iteratively training the policy in states induced by all previously learned policies.


%For example, during training one
%can use a “mixture oracle” that at times takes an action given by the previous learned policy [11].
%Alternatively, at each iteration one can learn a policy from trajectories generated by all previous policies [3].





\begin{algorithm}[htb]
 \KwData{}
 \KwResult{Best $\hat{\pi}_i$ on validation }
 Initialized $D \leftarrow \emptyset$ \\
 Initialized $\hat{\pi}_1$ to any policy in $\Pi$ \\
 \For{i=1 to N }{
	Let $\pi_i$ = $\beta_{i}\pi^* + (1-\beta)\hat{\pi_i}$ \\
	Sample T-step trajectories using $\pi_i$ \\
	Get dataset $D_i$ = \{(s, $\pi^*$(s))\} of visited states by $\pi_i$ and actions given by expert. \\
	Aggregate datasets: $D \leftarrow D \cup D_i$ \\
	Train classifier $\hat{\pi}_{i+1}$ on $D$\\
 }
 \caption{DAgger algorithms}
\end{algorithm}

\section{LEARNING EMBEDDED PARAMETERS}



\section{EXPERIMENTS}


%1 etot 
%2 convergence rate (L1 norm with previous iteration)
\subsection{Experiment settings}

	
What is the dataset ??\\
Training dataset, Testing dataset.... \\

How to evaluate the result?
What is "etot" \\
What is convergence rate \\

	

\subsection{Experiment result}



\section{GENERAL FORMATTING INSTRUCTIONS}



Both submitted and camera-ready versions of the paper are 8 pages,
plus any additional pages needed for references.

Papers are in 2 columns with the overall line width of 6.75~inches (41~picas). Each column is 3.25~inches wide (19.5~picas).  The space
between the columns is .25~inches wide (1.5~picas).  The left margin is 1~inch (6~picas).  Use 10~point type with a vertical spacing of
11~points. Times Roman is the preferred typeface throughout.

Paper title is 16~point, caps/lc, bold, centered between 2~horizontal rules.  Top rule is 4~points thick and bottom rule is 1~point thick.
Allow 1/4~inch space above and below title to rules.

Author descriptions are center-justified, initial caps.  The lead
author is to be listed first (left-most), and the Co-authors are set
to follow.  If up to three authors, use a single row of author
descriptions, each one center-justified, and all set side by side;
with more authors or unusually long names or institutions, use more
rows.  (But, do not include author names in the initial double-blind
submission!  Instead leave a row of ``Anonymous Author'' descriptions
as above.)


One-half line space between paragraphs, with no indent.

\section{FIRST LEVEL HEADINGS}

First level headings are all caps, flush left, bold, and in point size
12. One line space before the first level heading and 1/2~line space
after the first level heading.

\subsection{Second Level Heading}

Second level headings are initial caps, flush left, bold, and in point
size 10. One line space before the second level heading and 1/2~line
space after the second level heading.

\subsubsection{Third Level Heading}

Third level headings are flush left, initial caps, bold, and in point
size 10. One line space before the third level heading and 1/2~line
space after the third level heading.

\paragraph{Fourth Level Heading}

Fourth level headings must be flush left, initial caps, bold, and
Roman type.  One line space before the fourth level heading, and
place the section text immediately after the heading with, no line
break, but an 11 point horizontal space.

\subsection{CITATIONS, FIGURES, REFERENCES}


\subsubsection{Citations in Text}

Citations within the text should include the author's last name and
year, e.g., (Cheesman, 1985). References should follow any style that
you are used to using, as long as their style is consistent throughout
the paper.  Be sure that the sentence reads correctly if the citation
is deleted: e.g., instead of ``As described by (Cheesman, 1985), we
first frobulate the widgets,'' write ``As described by Cheesman
(1985), we first frobulate the widgets.''  Be sure to avoid
accidentally disclosing author identities through citations.

\subsubsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first
  footnote.} in the text. Use 8 point type for footnotes. Place the
footnotes at the bottom of the column in which their markers appear,
continuing to the next column if required. Precede the footnote
section of a column with a 0.5 point horizontal rule 1~inch (6~picas)
long.\footnote{Sample of the second footnote.}

\subsubsection{Figures}

All artwork must be centered, neat, clean, and legible.  All lines
should be very dark for purposes of reproduction, and art work should
not be hand-drawn.  Figures may appear at the top of a column, at the
top of a page spanning multiple columns, inline within a column, or
with text wrapped around them, but the figure number and caption
always appear immediately below the figure.  Leave 2 line spaces
between the figure and the caption. The figure caption is initial caps
and each figure should be numbered consecutively.

Make sure that the figure caption does not get separated from the
figure. Leave extra white space at the bottom of the page rather than
splitting the figure and figure caption.
\begin{figure}[h]
\vspace{.3in}
\centerline{\fbox{This figure intentionally left non-blank}}
\vspace{.3in}
\caption{Sample Figure Caption}
\end{figure}

\subsubsection{Tables}

All tables must be centered, neat, clean, and legible. Do not use hand-drawn tables. Table number and title always appear above the table.
See Table~\ref{sample-table}.

One line space before the table title, one line space after the table title, and one line space after the table. The table title must be
initial caps and each table numbered consecutively.

\begin{table}[h]
\caption{Sample Table Title} \label{sample-table}
\begin{center}
\begin{tabular}{ll}
{\bf PART}  &{\bf DESCRIPTION} \\
\hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{SUPPLEMENTARY MATERIAL}

If you need to include additional appendices during submission, you
can include them in the supplementary material file.


\newpage

\section{INSTRUCTIONS FOR CAMERA-READY PAPERS}

For the camera-ready paper, if you are using \LaTeX, please make sure
that you follow these instructions.  (If you are not using \LaTeX,
please make sure to achieve the same effect using your chosen
typesetting package.)

\begin{enumerate}
    \item Install the package \texttt{fancyhdr.sty}. The
    \texttt{aistats2014.sty} file will make use of it.
    \item Begin your document with
    \begin{flushleft}
    \texttt{\textbackslash documentclass[twoside]\{article\}}\\
    \texttt{\textbackslash usepackage[accepted]\{aistats2014\}}
    \end{flushleft}
    The \texttt{twoside} option for the class article allows the
    package \texttt{fancyhdr.sty} to include headings for even and odd
    numbered pages. The option \texttt{accepted} for the package
    \texttt{aistats2014.sty} will write a copyright notice at the end of
    the first column of the first page. This option will also print
    headings for the paper.  For the \emph{even} pages, the title of
    the paper will be used as heading and for \emph{odd} pages the
    author names will be used as heading.  If the title of the paper
    is too long or the number of authors is too large, the style will
    print a warning message as heading. If this happens additional
    commands can be used to place as headings shorter versions of the
    title and the author names. This is explained in the next point.
    \item  If you get warning messages as described above, then
    immediately after $\texttt{\textbackslash
    begin\{document\}}$, write
    \begin{flushleft}
    \texttt{\textbackslash runningtitle\{Provide here an alternative shorter version of the title of your
    paper\}}\\
    \texttt{\textbackslash runningauthor\{Provide here the surnames of the authors of your paper, all separated by
    commas\}}
    \end{flushleft}
    The text that appears as argument in \texttt{\textbackslash
      runningtitle} will be printed as a heading in the \emph{even}
    pages. The text that appears as argument in \texttt{\textbackslash
      runningauthor} will be printed as a heading in the \emph{odd}
    pages.  If even the author surnames do not fit, it is acceptable
    to give a subset of author names followed by ``et al.''

    \item Use the file sample\_paper.tex as an example.

    \item Both submitted and camera-ready versions of the paper are 8
      pages, plus any additional pages needed for references.

    \item If you need to include additional appendices,
      you can include them in the supplementary
      material file.

    \item Please, don't change the layout given by the above
      instructions and by the style file.

\end{enumerate}

\subsubsection*{Acknowledgements}

Use unnumbered third level headings for the acknowledgements.  All
acknowledgements go at the end of the paper.  Be sure to omit any
identifying information in the initial double-blind submission!


\subsubsection*{References}

References follow the acknowledgements.  Use an unnumbered third level
heading for the references section.  Any choice of citation style is
acceptable as long as you are consistent.  Please use the same font
size for references as for the body of the paper---remember that
references do not count against your page length total.

J.~Alspector, B.~Gupta, and R.~B.~Allen (1989). Performance of a
stochastic learning microchip.  In D. S. Touretzky (ed.), {\it
  Advances in Neural Information Processing Systems 1}, 748-760.  San
Mateo, Calif.: Morgan Kaufmann.

F.~Rosenblatt (1962). {\it Principles of Neurodynamics.} Washington,
D.C.: Spartan Books.

G.~Tesauro (1989). Neurogammon wins computer Olympiad.  {\it Neural
  Computation} {\bf 1}(3):321-323.

\begin{thebibliography}{9}
\bibitem{Matteus}
  Matteus Tanha, Shiva Kaul, Alex Cappiello, Geoffrey J. Gordon, David J. Yaron.
  \emph{Embedding parameters in ab initio theory to develop well-controlled approximations based on molecular similarity}.
  Technical report arXiv:1311.3440.
  
  \bibitem{DAgger}
  Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell.
  \emph{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}.
  Technical report arXiv:1011.0686, arXiv, 2011.
  
  \bibitem{DAggerCompare}
  Andreas Vlachos.
  \emph{An investigation of imitation learning algorithms for structured prediction}.
  
  \bibitem{Ross}
    Ross and J. A. Bagnell.
  \emph{Efficient reductions for imitation
learning.} Proceedings of the 13th International
Conference on Artificial Intelligence and Statistics (AISTATS),
2010.
  
\end{thebibliography}

\end{document}
